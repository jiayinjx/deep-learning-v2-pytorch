{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network\n",
    "Here I'm just going to use the same model as in part 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layer: list of integers, the sizes of the hidden layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn. Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)   \n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.769..  Test Loss: 0.993..  Test Accuracy: 0.660\n",
      "Epoch: 1/2..  Training Loss: 1.049..  Test Loss: 0.741..  Test Accuracy: 0.722\n",
      "Epoch: 1/2..  Training Loss: 0.870..  Test Loss: 0.687..  Test Accuracy: 0.739\n",
      "Epoch: 1/2..  Training Loss: 0.761..  Test Loss: 0.668..  Test Accuracy: 0.751\n",
      "Epoch: 1/2..  Training Loss: 0.739..  Test Loss: 0.612..  Test Accuracy: 0.762\n",
      "Epoch: 1/2..  Training Loss: 0.729..  Test Loss: 0.633..  Test Accuracy: 0.773\n",
      "Epoch: 1/2..  Training Loss: 0.644..  Test Loss: 0.610..  Test Accuracy: 0.765\n",
      "Epoch: 1/2..  Training Loss: 0.675..  Test Loss: 0.572..  Test Accuracy: 0.790\n",
      "Epoch: 1/2..  Training Loss: 0.628..  Test Loss: 0.576..  Test Accuracy: 0.789\n",
      "Epoch: 1/2..  Training Loss: 0.639..  Test Loss: 0.564..  Test Accuracy: 0.798\n",
      "Epoch: 1/2..  Training Loss: 0.632..  Test Loss: 0.551..  Test Accuracy: 0.799\n",
      "Epoch: 1/2..  Training Loss: 0.661..  Test Loss: 0.530..  Test Accuracy: 0.798\n",
      "Epoch: 1/2..  Training Loss: 0.610..  Test Loss: 0.517..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.632..  Test Loss: 0.519..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.587..  Test Loss: 0.502..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.619..  Test Loss: 0.512..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.512..  Test Accuracy: 0.818\n",
      "Epoch: 1/2..  Training Loss: 0.563..  Test Loss: 0.500..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.497..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.576..  Test Loss: 0.492..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.517..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.573..  Test Loss: 0.502..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.495..  Test Accuracy: 0.821\n",
      "Epoch: 2/2..  Training Loss: 0.576..  Test Loss: 0.478..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.478..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.580..  Test Loss: 0.498..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.470..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.512..  Test Loss: 0.471..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.513..  Test Loss: 0.489..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.543..  Test Loss: 0.486..  Test Accuracy: 0.819\n",
      "Epoch: 2/2..  Training Loss: 0.536..  Test Loss: 0.461..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.500..  Test Loss: 0.457..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.464..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.560..  Test Loss: 0.461..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.450..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.523..  Test Loss: 0.443..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.463..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.458..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.563..  Test Loss: 0.460..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.550..  Test Loss: 0.453..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.456..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.461..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.457..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.572..  Test Loss: 0.455..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.496..  Test Loss: 0.448..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.434..  Test Accuracy: 0.840\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/Udacity_PyTorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JL added:\n",
    "Basically it says, inconsistent tensor size, expected tensor, this size but the sources is dense. When we trained this state_dict with this checkpoint, we had a different network architecture. In the original network we had 500 units in the first layer, and now we have 400 units in the first layer. \n",
    "So the thing is that when you build your network and you train it, when you load the state_dict back up, it ahs to go to a model with exactly the same architecture. \n",
    "So if you have code that can generate networks with different architecture when you load your state_dict back up, you also have to include information about the architecture. \n",
    "For instance, here with this network since we have arbitrary number of hidden layers, we need to include this information about the hidden layers in our checkpoint that we saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "# Save the dictionary\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember here that 'model.hidden_layers' is a list of these linear operations. Basically, hidden_layers is a list of the layers in the network. So, to get the actual values I had set for the hidden_layers depending on then number of units, I just call 'out_features' on each of those layers. So, this is basically just going through each of the layers in the list of hidden layers and then getting the number of features or units in that layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    # Pass in all the necessary arguments parameters to create our model\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that every parameter that you use for building your network, you hav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
